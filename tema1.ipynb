{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------- PARTE 1 -------------------------------------------\n",
    "\n",
    "Modelo basico que responde a perguntas de python\n",
    "\n",
    "Aqui podemos ver as dependencies que precisamos no nosso projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch torchvision huggingface_hub mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois vamos nos authneticar e carregar o modelo que queremos usar no nosso projeto, nesse caso aqui é o gemma 2B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authentifiez-vous avec votre token Hugging Face\n",
    "token = '' # por aqui o token do Hugging Face\n",
    "login(token=token)\n",
    "\n",
    "# Charger le modèle et le tokenizer\n",
    "model_name = \"google/gemma-2b\"  # Nom du modèle GEMMA 2B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida vamos declarar a nossa funçao de resposta que usa o que lhe mandamos em output para gerir uma resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=150):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui abaixo temos os nossos prompts/perguntas para fazer ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de prompt pour analyse de code\n",
    "prompt_code_analysis = \"Analyze the following Python code:\\n\\n\"\n",
    "code_example = \"\"\"\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add(5, 3)\n",
    "print(result)\n",
    "\"\"\"\n",
    "response = generate_response(prompt_code_analysis + code_example)\n",
    "print(response)\n",
    "\n",
    "# Exemple de prompt pour questions sur Python\n",
    "prompt_question = \"How to declare a variable in python language?\\n\\n\"\n",
    "response = generate_response(prompt_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------- PARTE 2 ------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Aqui nesta segunda parte, nos vamos tentar fazer um segundo modelo onde treinamos esse mesmo para ter respostas mais concretas e detalhadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das bibliotecas necessárias\n",
    "%pip install transformers datasets torch huggingface_hub pandas accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos declarar todas as bibliotecas que necessitamos e entao declarar o nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Autenticação com o token Hugging Face\n",
    "token = '' # por aqui o token do Hugging Face\n",
    "login(token=token)\n",
    "\n",
    "# Carregar o modelo e o tokenizer\n",
    "model_name = \"google/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converter os ficheiros recuperados do dataset em ficheiros json\n",
    "\n",
    "O dataset que tentei usar é muito voluminozo entao esta aqui o link do dataset\n",
    "https://www.kaggle.com/datasets/stackoverflow/pythonquestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins de fichiers\n",
    "zip_file_path = r'C:\\Users\\ricar\\Downloads\\archive.zip'  # Remplacez par le chemin de votre fichier ZIP\n",
    "extract_dir = r'C:\\Users\\ricar\\Documents\\Aulas_Portugal\\Inteligencia artificial\\tema1'  # Remplacez par le chemin du répertoire d'extraction\n",
    "json_dir = r'C:\\Users\\ricar\\Documents\\Aulas_Portugal\\Inteligencia artificial\\tema1\\json'  # Remplacez par le chemin de sauvegarde du fichier JSON\n",
    "\n",
    "# Créer le répertoire d'extraction et de sauvegarde JSON s'ils n'existent pas\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "# Extraire le fichier ZIP\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Lister les fichiers extraits\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "print(\"Fichiers extraits:\", extracted_files)\n",
    "\n",
    "# Traiter chaque fichier CSV extrait\n",
    "for file_name in extracted_files:\n",
    "    if file_name.endswith('.csv'):\n",
    "        csv_file_path = os.path.join(extract_dir, file_name)\n",
    "        \n",
    "        # Charger le fichier CSV\n",
    "        df = pd.read_csv(csv_file_path, encoding='latin1')\n",
    "        \n",
    "        # Convertir le DataFrame en JSON\n",
    "        json_file_path = os.path.join(json_dir, file_name.replace('.csv', '.json'))\n",
    "        df.to_json(json_file_path, orient='records', lines=True)\n",
    "        \n",
    "        print(f'Dataset sauvegardé en JSON à {json_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar e preparar os fichieros para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos para os arquivos JSON\n",
    "json_files = {\n",
    "    \"Answers\": r\"C:\\Users\\ricar\\Documents\\Aulas_Portugal\\Inteligencia artificial\\tema1\\json\\Answers.json\",\n",
    "    \"Questions\": r\"C:\\Users\\ricar\\Documents\\Aulas_Portugal\\Inteligencia artificial\\tema1\\json\\Questions.json\",\n",
    "    \"Tags\": r\"C:\\Users\\ricar\\Documents\\Aulas_Portugal\\Inteligencia artificial\\tema1\\json\\Tags.json\"\n",
    "}\n",
    "\n",
    "# Carregar os arquivos JSON\n",
    "answers_df = pd.read_json(json_files[\"Answers\"], lines=True)\n",
    "questions_df = pd.read_json(json_files[\"Questions\"], lines=True)\n",
    "tags_df = pd.read_json(json_files[\"Tags\"], lines=True)\n",
    "\n",
    "# Unir os dados conforme necessário (ajustar conforme sua necessidade)\n",
    "merged_df = questions_df.merge(answers_df, on=\"Id\")  # Exemplo de junção por \"Id\"\n",
    "\n",
    "# Criar um Dataset do Hugging Face a partir do DataFrame\n",
    "dataset = Dataset.from_pandas(merged_df)\n",
    "\n",
    "# Dividir o dataset em treino e validação\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Body'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "# trainer.save_model(\"caminho_para_salvar_seu_modelo\")\n",
    "\n",
    "# Avaliar o modelo\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiçao da funçao de geraçao e prompts de perguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar resposta\n",
    "def generate_response(prompt, max_length=150, temperature=0.7, top_p=0.9, repetition_penalty=2.0, no_repeat_ngram_size=2):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        temperature=temperature, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty, \n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Exemplo de prompt para análise de código\n",
    "prompt_code_analysis = \"Analyze the following Python code:\\n\\n\"\n",
    "code_example = \"\"\"\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add(5, 3)\n",
    "print(result)\n",
    "\"\"\"\n",
    "response = generate_response(prompt_code_analysis + code_example)\n",
    "print(\"Code Analysis Response:\\n\", response)\n",
    "\n",
    "# Exemplo de prompt para perguntas sobre Python\n",
    "prompt_question = \"Explain how to declare a variable in Python.\\n\"\n",
    "response = generate_response(prompt_question)\n",
    "print(\"Question Response:\\n\", response)\n",
    "print(\"End of the response\\n\")\n",
    "\n",
    "# Teste com mais exemplos\n",
    "prompt_question = \"What is a function in Python?\\n\"\n",
    "response = generate_response(prompt_question)\n",
    "print(\"Question Response:\\n\", response)\n",
    "print(\"End of the response\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
